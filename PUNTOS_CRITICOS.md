# Puntos Cr√≠ticos - Integraci√≥n N8N-Ollama

‚ö†Ô∏è **IMPORTANTE**: Esta configuraci√≥n ha sido verificada solo en entorno local (Docker Desktop).
No se ha probado en entornos de producci√≥n o servicios cloud.

## üõ†Ô∏è Entorno de Desarrollo y Despliegue

### Entorno Local Verificado
```yaml
sistema_base:
  os: Windows 11 Pro
  wsl: WSL2
  docker: Docker Desktop 4.x

comandos_desarrollo:
  build: docker-compose build --no-cache
  start: docker-compose up
  stop: docker-compose down
  logs: docker-compose logs -f

volumenes_persistentes:
  - ./.n8n:/home/node/.n8n
  - ./.ollama:/root/.ollama

puertos_expuestos:
  n8n: "5678:5678"
  ollama: "11434:11434"

recursos_recomendados:
  cpu: "2+ cores"
  ram: "4+ GB"
  storage: "10+ GB"

## üîç Entornos

### Desarrollo (‚úÖ VERIFICADO)
```yaml
entorno: "development"
plataforma: "Docker Desktop"
sistema_base: "Windows 11 + WSL2"
variables_entorno:
  - N8N_ENCRYPTION_KEY: [generado-din√°micamente]
  - OLLAMA_HOST: 0.0.0.0
  - OLLAMA_ORIGINS: *
  # ... otras variables de desarrollo

puertos:
  n8n: 5678
  ollama: 11434

volumenes:
  - ./.n8n:/home/node/.n8n
  - ./.ollama:/root/.ollama
```

### Producci√≥n (‚ö†Ô∏è PENDIENTE)
```yaml
entorno: "production"
plataforma: "Render.com"
variables_requeridas:
  N8N_ENCRYPTION_KEY:
    descripcion: "Clave de encriptaci√≥n para N8N"
    tipo: "secret"
    generacion: "openssl rand -hex 24"
  
  N8N_AUTH_USERNAME:
    descripcion: "Usuario administrador de N8N"
    tipo: "secret"
    ejemplo: "admin"
  
  N8N_AUTH_PASSWORD:
    descripcion: "Contrase√±a del usuario admin"
    tipo: "secret"
    requisitos: "M√≠nimo 8 caracteres"
  
  OLLAMA_ORIGINS:
    descripcion: "Or√≠genes permitidos para Ollama"
    valor: "https://tu-dominio.onrender.com"

seguridad:
  - TLS/SSL habilitado
  - Autenticaci√≥n b√°sica requerida
  - Tokens JWT para API
  - L√≠mites de rate

backups:
  - Base de datos: Diario
  - Workflows: Autom√°tico
  - Modelos: Manual
```

### Despliegue en Render.com üöÄ

#### Requisitos Previos
```yaml
servicios_necesarios:
  - Web Service:
      tipo: "Docker"
      plan: "Starter Plus" # M√≠nimo por requisitos de memoria
      regi√≥n: "Frankfurt (EU)"
      ram: "4GB"
      cpu: "2x"

configuraci√≥n_render:
  dockerfile_path: "./Dockerfile"
  docker_command: "/docker-entrypoint.sh"
  auto_deploy: true
  branch: "main"
```

#### Variables de Entorno Requeridas
```yaml
# Variables Cr√≠ticas (Deben configurarse en Render)
seguridad:
  N8N_BASIC_AUTH_ACTIVE: "true"
  N8N_BASIC_AUTH_USER: "admin"
  N8N_BASIC_AUTH_PASSWORD: "[contrase√±a-segura]"
  N8N_ENCRYPTION_KEY: "[generado-con-openssl]"
  N8N_JWT_SECRET: "[generado-con-openssl]"
  N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"

configuraci√≥n:
  NODE_ENV: "production"
  N8N_HOST: "0.0.0.0"
  N8N_PORT: "3000"  # Render asigna el puerto 3000
  N8N_PROTOCOL: "https"
  N8N_DISABLE_TUNNEL: "true"
  N8N_METRICS_DISABLED: "true"
  N8N_DIAGNOSTICS_DISABLED: "true"
  N8N_USER_FOLDER: "/home/node/.n8n"
  WEBHOOK_URL: "https://[tu-app].onrender.com"
  N8N_LOG_LEVEL: "verbose"
  N8N_SKIP_WEBHOOK_DEREGISTRATION: "true"

ollama:
  OLLAMA_HOST: "127.0.0.1"  # Cr√≠tico: En Render debe ser 127.0.0.1
  OLLAMA_ORIGINS: "https://[tu-app].onrender.com"
  OLLAMA_MODELS: "llama3.2:1b"
  OLLAMA_SKIP_GPU_DETECTION: "true"
  OLLAMA_CPU_ONLY: "true"

# Variable de Control de Entorno
control:
  RENDER: "true"  # Indica que estamos en entorno Render
```

#### Ajustes Necesarios
```yaml
dockerfile:
  - Usar puerto 3000 en lugar de 5678
  - Asegurar persistencia en /data
  - Optimizar capas para reducir tama√±o

entrypoint:
  - Verificar conexi√≥n PostgreSQL
  - Manejar migraciones DB
  - Ajustar timeouts de inicio

seguridad:
  - Habilitar autenticaci√≥n b√°sica
  - Configurar HTTPS
  - Limitar or√≠genes CORS
```

#### Orden de Despliegue
1. Crear base de datos PostgreSQL
2. Obtener credenciales y URL de conexi√≥n
3. Configurar variables de entorno
4. Desplegar Web Service
5. Verificar logs y estado

#### Monitoreo en Producci√≥n
```yaml
puntos_cr√≠ticos:
  - Uso de memoria
  - Conexi√≥n a PostgreSQL
  - Estado de Ollama
  - Logs de N8N

m√©tricas:
  - Uso de CPU/RAM
  - Tiempo de respuesta
  - Errores HTTP
  - Estado de modelos
```

### ‚ö†Ô∏è Limitaciones Conocidas
1. Docker Desktop:
   - Rendimiento puede variar en WSL2
   - Necesita virtualizaci√≥n habilitada
   - Consumo de memoria significativo

2. Desarrollo Local:
   - No probado en producci√≥n
   - Sin configuraci√≥n de alta disponibilidad
   - Sin estrategias de backup autom√°tico

3. Seguridad:
   - Puertos expuestos localmente
   - Sin TLS/SSL configurado
   - Autenticaci√≥n b√°sica

### üîÑ Ciclo de Desarrollo
```mermaid
graph TD
    A[Modificar C√≥digo] --> B[Build sin cach√©]
    B --> C[Iniciar Servicios]
    C --> D{Verificar Logs}
    D -->|Error| A
    D -->|Success| E[Pruebas]
    E --> F{Actualizar Docs}
```

## üîí Puntos Cr√≠ticos para Modificaciones

Este documento debe ser revisado antes de cualquier modificaci√≥n al sistema. Los siguientes puntos son cr√≠ticos para el funcionamiento correcto de la integraci√≥n N8N-Ollama.

### Entorno de Pruebas Verificado üß™
```yaml
plataforma:
  - sistema: Windows 11 Pro
  - docker: Docker Desktop 4.x
  - runtime: WSL2

recursos_m√≠nimos:
  cpu: 2 cores
  memoria: 4GB
  disco: 10GB

pruebas_realizadas:
  - Inicio/parada de servicios
  - Descarga de modelos
  - Comunicaci√≥n entre servicios
  - Persistencia de datos

entornos_no_verificados:
  - Kubernetes
  - Servicios Cloud (AWS, GCP, Azure)
  - Linux nativo
  - macOS
```

### 1. Rutas y Binarios Verificados ‚ö†Ô∏è
```plaintext
- Ollama binario: /usr/bin/ollama (desde imagen ollama/ollama:latest)
- N8N binario: Usar Node.js 20.x con npm 10.2.4
```

### 2. Estructura de Permisos üîê
```plaintext
N8N:
- Directorio: /home/node/.n8n
- Permisos: 750
- Usuario: node:node

Ollama:
- Directorio: /root/.ollama
- Permisos: 755
- Usuario: root:root
```

### 3. Configuraci√≥n de Red üåê
```plaintext
Ollama:
- OLLAMA_HOST=0.0.0.0
- OLLAMA_ORIGINS=*
- Puerto interno: 11434

N8N:
- N8N_HOST=0.0.0.0
- Puerto interno: 5678
```

### 4. Orden de Inicio de Servicios ‚ö°
```plaintext
1. Configurar permisos de directorios
2. Iniciar Ollama como root
3. Esperar respuesta de Ollama en 127.0.0.1:11434
4. Iniciar N8N como usuario node
```

### 5. Archivos Cr√≠ticos üìÑ
```plaintext
- Dockerfile: Construcci√≥n multi-etapa con ollama/ollama:latest
- docker-entrypoint.sh: Secuencia de inicio y permisos
- docker-compose.yml: Configuraci√≥n de red y vol√∫menes
```

### 6. Versiones Verificadas üìå
```plaintext
- N8N: 1.76.1
- Ollama: latest (desde imagen oficial)
- Ubuntu: 22.04 (base)
- Node.js: 20.x
- npm: 10.2.4
```

### 7. Configuraci√≥n Base Verificada üîß
```dockerfile
# Multi-stage build
FROM ollama/ollama:latest AS ollama
FROM ubuntu:22.04

# Dependencias base
RUN apt-get update && apt-get install -y \
    curl wget python3 ca-certificates \
    gosu procps bash uuid-runtime openssl

# Node.js y npm
RUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
    apt-get install -y nodejs && \
    npm install -g npm@10.2.4

# N8N instalaci√≥n
RUN npm install -g n8n@1.76.1 --legacy-peer-deps
```

### 8. Variables de Entorno Cr√≠ticas üîê
```env
# Ollama
OLLAMA_HOST=0.0.0.0
OLLAMA_ORIGINS=*
OLLAMA_SKIP_GPU_DETECTION=true
OLLAMA_CPU_ONLY=true
CGO_ENABLED=0

# N8N
NODE_ENV=production
N8N_ENCRYPTION_KEY=[generado-din√°micamente]
N8N_DISABLE_TUNNEL=true
N8N_METRICS_DISABLED=true
N8N_DIAGNOSTICS_DISABLED=true
N8N_HOST=0.0.0.0
N8N_PORT=5678
```

### 9. Secuencia de Inicio Verificada üîÑ
```bash
# 1. Preparaci√≥n de directorios
mkdir -p /home/node/.n8n
find /home/node/.n8n -mindepth 1 -delete 2>/dev/null || true
mkdir -p /home/node/.n8n/.n8n
mkdir -p /home/node/.n8n/data
chown -R node:node /home/node/.n8n
chmod -R 750 /home/node/.n8n

# 2. Inicio de Ollama
/usr/local/bin/ollama serve &
# Esperar respuesta en puerto 11434

# 3. Inicio de N8N
cd /home/node
gosu node n8n start
# Verificar respuesta en puerto 5678
```

### 10. Soluci√≥n de Problemas Comunes üîß
1. Error "Device or resource busy":
   - No eliminar directorio ra√≠z montado
   - Usar find para limpiar contenido

2. Error de inicio N8N:
   - Evitar flags de l√≠nea de comando
   - Usar variables de entorno
   - Asegurar permisos correctos

3. Problemas de bibliotecas:
   - Usar Ubuntu 22.04 como base
   - Instalar dependencias del sistema
   - Verificar versiones de Node.js y npm

## ‚ö†Ô∏è Proceso de Modificaci√≥n

1. **Antes de modificar:**
   - Revisar este documento
   - Verificar que los cambios no afectan puntos cr√≠ticos
   - Hacer backup de configuraci√≥n actual

2. **Durante la modificaci√≥n:**
   - Mantener estructura de permisos
   - Respetar orden de inicio de servicios
   - No alterar rutas verificadas

3. **Despu√©s de modificar:**
   - Verificar todos los puntos cr√≠ticos
   - Probar integraci√≥n completa
   - Actualizar documentaci√≥n si es necesario

## üö´ Cambios Prohibidos
- No cambiar usuarios de ejecuci√≥n
- No modificar estructura de permisos
- No alterar orden de inicio de servicios
- No cambiar rutas de binarios verificados

## ‚úÖ Verificaci√≥n de Funcionamiento
```bash
# Verificar Ollama
curl http://localhost:11434/api/version

# Verificar N8N
curl http://localhost:5678/healthz
```

---
*√öltima actualizaci√≥n: 2025-02-08*

*Este documento debe ser revisado y actualizado con cada modificaci√≥n exitosa del sistema.*

# Puntos Cr√≠ticos del Sistema

## 1. Ollama (‚úÖ RESUELTO)

### Dependencias Cr√≠ticas
- Bibliotecas necesarias en Ubuntu:
  ```dockerfile
  RUN apt-get update && apt-get install -y \
      curl \
      wget \
      python3 \
      ca-certificates \
      gosu \
      procps \
      bash
  ```

### Estructura de Directorios
- Crear directorios necesarios:
  ```dockerfile
  RUN mkdir -p /usr/local/bin /lib/x86_64-linux-gnu
  ```

### Bibliotecas Compartidas
- Copiar solo las bibliotecas esenciales:
  ```dockerfile
  COPY --from=ollama /usr/lib/x86_64-linux-gnu/libstdc++.so* /usr/lib/x86_64-linux-gnu/
  COPY --from=ollama /usr/lib/x86_64-linux-gnu/libgcc_s.so* /usr/lib/x86_64-linux-gnu/
  ```

### Correcci√≥n de Enlaces Simb√≥licos
```dockerfile
RUN cd /usr/lib/x86_64-linux-gnu && \
    rm -f libstdc++.so.6 && \
    ln -s libstdc++.so.6.* libstdc++.so.6
```

### Variables de Entorno
```env
OLLAMA_HOST=0.0.0.0
OLLAMA_ORIGINS=*
OLLAMA_SKIP_GPU_DETECTION=true
OLLAMA_CPU_ONLY=true
CGO_ENABLED=0
```

### Diagn√≥stico de Problemas
1. Verificar que el binario es ejecutable:
   ```bash
   ls -l /usr/local/bin/ollama || ls -l /usr/bin/ollama
   ```

2. Verificar bibliotecas del sistema:
   ```bash
   ldd $(which ollama)
   ```

3. Verificar logs si falla:
   ```bash
   tail -n 50 /var/log/ollama/ollama.log
   ```

### Soluci√≥n de Problemas de Bibliotecas
1. Errores de relocaci√≥n:
   - Instalar las bibliotecas nativas de Alpine en lugar de copiarlas de Ollama
   - Usar las versiones -dev de las bibliotecas cuando sea necesario

2. S√≠mbolos no encontrados:
   - Verificar que todas las bibliotecas de desarrollo est√°n instaladas
   - Usar gcompat para compatibilidad con glibc

## 2. Directorios (‚úÖ RESUELTO)
- /root/.ollama: 755 (root:root)
- /home/node/.n8n: 750 (node:node)

## 3. Red (‚úÖ RESUELTO)
- Ollama: 0.0.0.0:11434
- N8N: 0.0.0.0:5678

## 4. Servicios
- Ollama: ‚úÖ RESUELTO
- N8N: ‚úÖ RESUELTO

### Configuraci√≥n de Modelos Ollama ü§ñ
```yaml
models:
  default: llama3.2:1b  # √önico modelo soportado actualmente
  config:
    llama3.2:1b:
      description: "Modelo LLaMA 3.2 1B - Versi√≥n ligera optimizada"
      pull_command: "ollama pull --insecure llama3.2:1b"
      verify_command: "curl -s http://127.0.0.1:11434/api/tags | grep -q \"llama3.2:1b\""
      notas:
        - "√önico modelo verificado y soportado"
        - "No descargar otros modelos autom√°ticamente"
        - "Otros modelos pueden causar problemas de memoria"
      parameters:
        context_length: 4096
        temperature: 0.7
        top_p: 0.9
        repeat_penalty: 1.1
```

#### Instrucciones de Uso
1. El modelo se descarga autom√°ticamente al inicio si no existe
2. Para descargar manualmente: `ollama pull llama3.2:1b`
3. Para verificar la instalaci√≥n: `ollama list`
4. Para actualizar el modelo: `ollama rm llama3.2:1b && ollama pull llama3.2:1b`

#### ‚ö†Ô∏è Importante
- No intentar descargar otros modelos
- No modificar el modelo por defecto
- Mantener solo llama3.2:1b instalado

### N8N - Lecciones Aprendidas
1. Configuraci√≥n:
   - Usar variables de entorno en lugar de flags de l√≠nea de comando
   ```bash
   export N8N_DISABLE_TUNNEL=true
   export N8N_METRICS_DISABLED=true
   export N8N_DIAGNOSTICS_DISABLED=true
   export N8N_ENCRYPTION_KEY=$(openssl rand -hex 24)
   ```

2. Estructura de Directorios:
   - No eliminar el directorio ra√≠z mientras est√° en uso
   - Usar find para limpiar contenido: `find /home/node/.n8n -mindepth 1 -delete`
   - Estructura m√≠nima necesaria:
     ```
     /home/node/.n8n/
     ‚îú‚îÄ‚îÄ .n8n/
     ‚îî‚îÄ‚îÄ data/
     ```

3. Inicio del Servicio:
   - Comando simple: `n8n start`
   - Evitar flags innecesarios
   - Usar gosu para cambio de usuario
   - Verificar el proceso despu√©s del inicio

4. Puntos Clave del √âxito:
   - Usar Ubuntu 22.04 como base en lugar de Alpine
   - Mantener la configuraci√≥n simple y basada en variables de entorno
   - No usar flags de l√≠nea de comando innecesarios
   - Asegurar permisos y propiedad de directorios
   - Limpiar contenido sin eliminar directorios montados

### Orden de Inicio
1. Configurar permisos
2. Configurar red
3. Iniciar Ollama
4. Verificar API de Ollama
5. Iniciar N8N

## 5. Gesti√≥n de Usuarios (‚úÖ RESUELTO)
```dockerfile
# Crear grupo y usuario node correctamente
RUN groupadd -r node && \
    useradd -r -u 1000 -g node -d /home/node -m -s /bin/bash node
```

## 6. Problemas Conocidos y Soluciones

### Ollama (‚úÖ RESUELTO)
1. Bibliotecas faltantes: Usar Ubuntu 22.04 como base
2. Enlaces simb√≥licos: Crear manualmente para libstdc++
3. Permisos: Asegurar 755 para binarios

### N8N (‚úÖ RESUELTO)
1. Error actual: --debug no es una opci√≥n v√°lida
2. Verificar la versi√≥n correcta de N8N
3. Revisar logs en /home/node/.n8n/

## 7. Script de Entrada (‚úÖ RESUELTO)
1. Configuraci√≥n Base:
   ```bash
   #!/bin/bash
   set -e
   set -m  # Habilitar control de trabajos
   ```

2. Manejo de Se√±ales:
   ```bash
   trap 'cleanup' SIGTERM SIGINT SIGHUP
   
   cleanup() {
       echo "Deteniendo servicios..."
       kill $OLLAMA_PID $N8N_PID
       wait $OLLAMA_PID $N8N_PID
       exit 0
   }
   ```

3. Monitoreo de Procesos:
   ```bash
   check_process() {
       if ! kill -0 $1 2>/dev/null; then
           echo "[$(date)] Proceso $2 (PID: $1) no est√° ejecut√°ndose"
           return 1
       fi
       return 0
   }
   ```

4. Secuencia de Verificaci√≥n:
   - Binarios y permisos
   - Bibliotecas del sistema
   - Configuraci√≥n de red
   - Inicio de servicios
   - Verificaci√≥n de salud

5. Logs y Diagn√≥stico:
   - Timestamps en todos los mensajes
   - Verificaci√≥n de procesos
   - Captura de errores
   - Logs espec√≠ficos por servicio

### Orden de Ejecuci√≥n Verificado
1. Verificaci√≥n de binarios y bibliotecas
2. Configuraci√≥n de permisos
3. Configuraci√≥n de red
4. Inicio de Ollama
5. Verificaci√≥n de API Ollama
6. Descarga de modelo (llama3.2:1b)
7. Inicio de N8N
8. Verificaci√≥n de salud N8N
9. Monitoreo continuo

## 8. Variables de Entorno Cr√≠ticas üîê
```env
# Variables Verificadas y Funcionales
# Ollama
OLLAMA_HOST=0.0.0.0
OLLAMA_ORIGINS=*
OLLAMA_SKIP_GPU_DETECTION=true
OLLAMA_CPU_ONLY=true
CGO_ENABLED=0

# N8N
NODE_ENV=production
N8N_ENCRYPTION_KEY=[generado-din√°micamente]
N8N_DISABLE_TUNNEL=true
N8N_METRICS_DISABLED=true
N8N_DIAGNOSTICS_DISABLED=true
N8N_HOST=0.0.0.0
N8N_PORT=5678
```

## 9. Modelos de IA Verificados ü§ñ
### Modelo Principal
```yaml
nombre: llama3.2:1b
estado: ‚úÖ Verificado
comando_descarga: ollama pull --insecure llama3.2:1b
verificaci√≥n: curl -s http://127.0.0.1:11434/api/tags | grep -q "llama3.2:1b"
```

### Gesti√≥n de Modelos
1. Descarga Autom√°tica:
   - Se intenta al inicio si no existe
   - Verificaci√≥n previa a la descarga
   - Manejo de errores de descarga

2. Verificaci√≥n:
   ```bash
   # Verificar modelo instalado
   ollama list
   
   # Verificar funcionamiento
   curl http://localhost:11434/api/generate -d '{
     "model": "llama3.2:1b",
     "prompt": "test"
   }'
   ```

## 10. Archivos del Proyecto üìÅ

### Archivos Principales
```yaml
docker:
  - Dockerfile: Configuraci√≥n multi-etapa para la imagen
  - docker-compose.yml: Orquestaci√≥n de servicios
  - docker-entrypoint.sh: Script principal de inicio

despliegue:
  - DEPLOYMENT_GUIDE.md: Gu√≠a de despliegue (pendiente)
  - render.yaml: Configuraci√≥n para Render (no verificado)
  - PUNTOS_CRITICOS.md: Este documento

scripts:
  - push-to-github.sh: Script Linux para subir cambios
  - push-to-github.ps1: Script PowerShell para subir cambios
```

### Estado de Verificaci√≥n
```yaml
verificados_local:
  - Dockerfile: ‚úÖ
  - docker-compose.yml: ‚úÖ
  - docker-entrypoint.sh: ‚úÖ

pendientes_verificacion:
  - render.yaml: ‚ùå No probado en producci√≥n
  - DEPLOYMENT_GUIDE.md: ‚ùå En desarrollo

scripts_auxiliares:
  - push-to-github.*: ‚ö†Ô∏è Solo para desarrollo
```

### Dependencias entre Archivos
```mermaid
graph TD
    A[Dockerfile] --> B[docker-entrypoint.sh]
    C[docker-compose.yml] --> A
    C --> D[Vol√∫menes]
    B --> E[Logs]
    F[render.yaml] -.-> A
    G[DEPLOYMENT_GUIDE.md] -.-> F
```

### Diferencias Cr√≠ticas Dev vs Prod
```diff
# Variable de Entorno Cr√≠tica
+ RENDER=true                   # Prod

# Variables de Entorno
- N8N_BASIC_AUTH_ACTIVE=false    # Dev
+ N8N_BASIC_AUTH_ACTIVE=true     # Prod

# Seguridad
- OLLAMA_ORIGINS=*               # Dev
+ OLLAMA_ORIGINS=https://...     # Prod
```

### Variables de Control de Entorno
```yaml
RENDER:
  descripci√≥n: "Variable cr√≠tica para diferenciar entorno"
  valores:
    - false: "Entorno de desarrollo local"
    - true: "Entorno de producci√≥n en Render.com"
  uso:
    - Controla flujo de inicio de servicios
    - Determina configuraci√≥n de base de datos
    - Activa/desactiva caracter√≠sticas de seguridad
  ejemplo:
    desarrollo: |
      RENDER=false docker-compose up
    producci√≥n: |
      RENDER=true en variables de entorno de Render.com
```

### Valores por Defecto (No usar en producci√≥n)
```yaml
N8N_ENCRYPTION_KEY: "dK8xJ2mP9nQ7vR4tL5wC3bE6yH8uM1pX"  # Solo desarrollo
N8N_BASIC_AUTH_USER: "admin"  # Cambiar en producci√≥n
N8N_BASIC_AUTH_PASSWORD: "admin123"  # Cambiar en producci√≥n
```

### Notas Importantes
```yaml
puertos:
  - N8N_PORT ser√° sobreescrito por $PORT en Render
  - Ollama debe usar 127.0.0.1 en producci√≥n

seguridad:
  - Generar claves seguras con openssl
  - No usar valores por defecto en producci√≥n
  - Habilitar autenticaci√≥n b√°sica

webhooks:
  - Configurar WEBHOOK_URL con la URL final de Render
  - Verificar redirecciones HTTPS
```

### Google Cloud Platform
```yaml
puntos_criticos:
  - Asegurar que el project_id est√° configurado correctamente
  - Verificar que la regi√≥n es correcta (europe-west1)
  - Los puertos 80 y 22 deben estar abiertos en el security list

recursos_minimos:
  cpu: 4 vCPUs
  ram: 16GB
  storage: 100GB

directorios:
  - /n8n-ollama/data/n8n
  - /n8n-ollama/data/ollama
``` 